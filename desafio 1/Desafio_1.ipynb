{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-06-19T19:37:33.992366Z",
          "start_time": "2025-06-19T19:37:29.192180Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C8SQ_xrntzD6",
        "outputId": "e93f1aa8-f2c0-4392-8050-85056396a4e2"
      },
      "source": [
        "%pip install numpy scikit-learn matplotlib pandas"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq6j8LsYq1Dr"
      },
      "source": [
        "### Vectorización de texto y modelo de clasificación Naïve Bayes con el dataset 20 newsgroups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7cXR6CI30ry",
        "ExecuteTime": {
          "end_time": "2025-06-19T19:37:47.281862Z",
          "start_time": "2025-06-19T19:37:36.233640Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7311ca7-98c0-4aa9-d388-4b9263a9af32"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
        "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "print(\"Librerías importadas exitosamente\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Librerías importadas exitosamente\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yD-pVDWV_rQc"
      },
      "source": [
        "## Carga de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ech9qJaUo9vK",
        "ExecuteTime": {
          "end_time": "2025-06-19T19:38:11.528808Z",
          "start_time": "2025-06-19T19:38:00.808990Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1988ea7b-0def-4030-e5c7-33f5c7674661"
      },
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"CARGA DE DATOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_test = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "ntrain = len(newsgroups_train.data)\n",
        "ntest = len(newsgroups_test.data)\n",
        "ndata = ntrain + ntest\n",
        "nclasses = len(newsgroups_train.target_names)\n",
        "\n",
        "print(f'Cantidad de datos de entrenamiento: {ntrain}')\n",
        "print(f'Cantidad de datos de prueba: {ntest}')\n",
        "print(f'Cantidad total de documentos: {ndata}')\n",
        "print(f'Número de clases: {nclasses}')\n",
        "print(f'Fracción de entrenamiento: {ntrain/ndata:.2f}')\n",
        "print(f'Fracción de prueba: {ntest/ndata:.2f}')\n",
        "\n",
        "print(\"\\nClases del dataset:\")\n",
        "for i, name in enumerate(newsgroups_train.target_names):\n",
        "    print(f\"{i:2d}. {name}\")\n",
        "\n",
        "print(f\"\\nEjemplo de documento (índice 0):\")\n",
        "print(f\"Clase: {newsgroups_train.target_names[newsgroups_train.target[0]]}\")\n",
        "print(f\"Texto: {newsgroups_train.data[0][:300]}...\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CARGA DE DATOS\n",
            "================================================================================\n",
            "Cantidad de datos de entrenamiento: 11314\n",
            "Cantidad de datos de prueba: 7532\n",
            "Cantidad total de documentos: 18846\n",
            "Número de clases: 20\n",
            "Fracción de entrenamiento: 0.60\n",
            "Fracción de prueba: 0.40\n",
            "\n",
            "Clases del dataset:\n",
            " 0. alt.atheism\n",
            " 1. comp.graphics\n",
            " 2. comp.os.ms-windows.misc\n",
            " 3. comp.sys.ibm.pc.hardware\n",
            " 4. comp.sys.mac.hardware\n",
            " 5. comp.windows.x\n",
            " 6. misc.forsale\n",
            " 7. rec.autos\n",
            " 8. rec.motorcycles\n",
            " 9. rec.sport.baseball\n",
            "10. rec.sport.hockey\n",
            "11. sci.crypt\n",
            "12. sci.electronics\n",
            "13. sci.med\n",
            "14. sci.space\n",
            "15. soc.religion.christian\n",
            "16. talk.politics.guns\n",
            "17. talk.politics.mideast\n",
            "18. talk.politics.misc\n",
            "19. talk.religion.misc\n",
            "\n",
            "Ejemplo de documento (índice 0):\n",
            "Clase: rec.autos\n",
            "Texto: I was wondering if anyone out there could enlighten me on this car I saw\n",
            "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
            "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
            "the front bumper was separate from the rest of the body. This is \n",
            "all I k...\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxjSI7su_uWI"
      },
      "source": [
        "## Vectorización"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-94VP0QYCzDn",
        "ExecuteTime": {
          "end_time": "2025-06-19T19:38:32.759830Z",
          "start_time": "2025-06-19T19:38:31.581544Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a3db5f-f605-4c27-aebe-e7059aa4a846"
      },
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"1. VECTORIZACIÓN Y ANÁLISIS DE SIMILARIDAD ENTRE DOCUMENTOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "tfidfvect = TfidfVectorizer()\n",
        "\n",
        "X_train = tfidfvect.fit_transform(newsgroups_train.data)\n",
        "y_train = newsgroups_train.target\n",
        "\n",
        "print(f'Forma de la matriz documento-término: {X_train.shape}')\n",
        "print(f'Tipo de matriz: {type(X_train)}')\n",
        "print(f'Tamaño del vocabulario: {X_train.shape[1]}')\n",
        "\n",
        "idx2word = {v: k for k, v in tfidfvect.vocabulary_.items()}\n",
        "\n",
        "def analizar_similaridad_documento(idx, nmost=5):\n",
        "    \"\"\"\n",
        "    Analiza la similaridad de un documento con el resto del corpus\n",
        "    \"\"\"\n",
        "    cossim = cosine_similarity(X_train[idx], X_train)[0]\n",
        "\n",
        "    most_similar = np.argsort(cossim)[::-1][1:nmost+1]\n",
        "\n",
        "    print(f\"\\nDocumento base (índice {idx}):\")\n",
        "    print(f\"Clase: {newsgroups_train.target_names[y_train[idx]]}\")\n",
        "    print(f\"Texto: {newsgroups_train.data[idx][:200]}...\")\n",
        "\n",
        "    print(f\"\\nLos {nmost} documentos más similares:\")\n",
        "    for i, sim_idx in enumerate(most_similar):\n",
        "        sim_score = cossim[sim_idx]\n",
        "        sim_class = newsgroups_train.target_names[y_train[sim_idx]]\n",
        "        print(f\"  {i+1}. Índice {sim_idx}, Similaridad: {sim_score:.4f}, Clase: {sim_class}\")\n",
        "        print(f\"     Texto: {newsgroups_train.data[sim_idx][:150]}...\")\n",
        "\n",
        "    base_class = newsgroups_train.target_names[y_train[idx]]\n",
        "    similar_classes = [newsgroups_train.target_names[y_train[i]] for i in most_similar]\n",
        "    matches = sum(1 for cls in similar_classes if cls == base_class)\n",
        "\n",
        "    print(f\"\\nAnálisis de clases:\")\n",
        "    print(f\"  - Clase del documento base: {base_class}\")\n",
        "    print(f\"  - Coincidencias exactas: {matches}/{nmost}\")\n",
        "    print(f\"  - Porcentaje de acierto: {matches/nmost*100:.1f}%\")\n",
        "\n",
        "    return most_similar, cossim[most_similar]\n",
        "\n",
        "random.seed(42)\n",
        "random_docs = random.sample(range(X_train.shape[0]), 5)\n",
        "\n",
        "print(f\"Documentos seleccionados para análisis: {random_docs}\")\n",
        "\n",
        "resultados_similaridad = {}\n",
        "for doc_idx in random_docs:\n",
        "    similar_indices, similarities = analizar_similaridad_documento(doc_idx)\n",
        "    resultados_similaridad[doc_idx] = {\n",
        "        'similar_indices': similar_indices,\n",
        "        'similarities': similarities\n",
        "    }\n",
        "    print(\"-\" * 80)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "1. VECTORIZACIÓN Y ANÁLISIS DE SIMILARIDAD ENTRE DOCUMENTOS\n",
            "================================================================================\n",
            "Forma de la matriz documento-término: (11314, 101631)\n",
            "Tipo de matriz: <class 'scipy.sparse._csr.csr_matrix'>\n",
            "Tamaño del vocabulario: 101631\n",
            "Documentos seleccionados para análisis: [10476, 1824, 409, 4506, 4012]\n",
            "\n",
            "Documento base (índice 10476):\n",
            "Clase: rec.sport.hockey\n",
            "Texto: This is a general question for US readers:\n",
            "\n",
            "How extensive is the playoff coverage down there?  In Canada, it is almost\n",
            "impossible not to watch a series on TV (ie the only two series I have not had\n",
            "an ...\n",
            "\n",
            "Los 5 documentos más similares:\n",
            "  1. Índice 5064, Similaridad: 0.2250, Clase: rec.sport.hockey\n",
            "     Texto: \n",
            "I only have one comment on this:  You call this a *classic* playoff year\n",
            "and yet you don't include a Chicago-Detroit series.  C'mon, I'm a Boston\n",
            "fan...\n",
            "  2. Índice 9623, Similaridad: 0.2174, Clase: talk.politics.mideast\n",
            "     Texto: Accounts of Anti-Armenian Human Right Violations in Azerbaijan #012\n",
            "                 Prelude to Current Events in Nagorno-Karabakh\n",
            "\n",
            "        +---------...\n",
            "  3. Índice 10575, Similaridad: 0.2164, Clase: sci.crypt\n",
            "     Texto: \n",
            "I am not an expert in the cryptography science, but some basic things\n",
            "seem evident to me, things which this Clinton Clipper do not address.\n",
            "The all p...\n",
            "  4. Índice 10836, Similaridad: 0.2126, Clase: alt.atheism\n",
            "     Texto: Archive-name: atheism/faq\n",
            "Alt-atheism-archive-name: faq\n",
            "Last-modified: 5 April 1993\n",
            "Version: 1.1\n",
            "\n",
            "                    Alt.Atheism Frequently-Asked Que...\n",
            "  5. Índice 2350, Similaridad: 0.2111, Clase: sci.crypt\n",
            "     Texto: Archive-name: net-privacy/part1\n",
            "Last-modified: 1993/3/3\n",
            "Version: 2.1\n",
            "\n",
            "\n",
            "IDENTITY, PRIVACY, and ANONYMITY on the INTERNET\n",
            "==============================...\n",
            "\n",
            "Análisis de clases:\n",
            "  - Clase del documento base: rec.sport.hockey\n",
            "  - Coincidencias exactas: 1/5\n",
            "  - Porcentaje de acierto: 20.0%\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Documento base (índice 1824):\n",
            "Clase: comp.sys.mac.hardware\n",
            "Texto: \n",
            "\n",
            "\tI think this kind of comparison is pretty useless in general.  The\n",
            "processor is only good when a good computer is designed around it adn the\n",
            "computer is used in its designed purpose.  Comparing pro...\n",
            "\n",
            "Los 5 documentos más similares:\n",
            "  1. Índice 9921, Similaridad: 0.3542, Clase: comp.sys.mac.hardware\n",
            "     Texto: dhk@ubbpc.uucp (Dave Kitabjian) writes ...\n",
            "\n",
            "040 486 030 386 020 286\n",
            "\n",
            "\n",
            "060 fastest, then Pentium, with the first versions of the PowerPC\n",
            "somewhere in t...\n",
            "  2. Índice 6364, Similaridad: 0.3132, Clase: comp.sys.mac.hardware\n",
            "     Texto: Well folks, after some thought the answer struck me flat in the face:\n",
            "\n",
            "\"Why would Apple release a Duo Dock with a processor of its own?\"\n",
            "\n",
            "Here's why- ...\n",
            "  3. Índice 5509, Similaridad: 0.3041, Clase: comp.sys.mac.hardware\n",
            "     Texto: rvenkate@ux4.cso.uiuc.edu (Ravikuma Venkateswar) writes ...\n",
            "\n",
            "Benchmarks are for marketing dweebs and CPU envy.  OK, if it will make\n",
            "you happy, the 486...\n",
            "  4. Índice 2641, Similaridad: 0.2504, Clase: comp.sys.mac.hardware\n",
            "     Texto: \n",
            "    I think this is mostly the fault of the people who write up the\n",
            "literature and price lists being confused themselves. Since there are\n",
            "two possibl...\n",
            "  5. Índice 4359, Similaridad: 0.2417, Clase: comp.sys.mac.hardware\n",
            "     Texto: If you get the Centris 650 with CD configuration, you are getting a Mac with\n",
            "a 68RC040 processor that has built-in math coprocessor support.  My \n",
            "unde...\n",
            "\n",
            "Análisis de clases:\n",
            "  - Clase del documento base: comp.sys.mac.hardware\n",
            "  - Coincidencias exactas: 5/5\n",
            "  - Porcentaje de acierto: 100.0%\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Documento base (índice 409):\n",
            "Clase: comp.graphics\n",
            "Texto: I can't fiqure this out.  I have properly compiled pov on a unix machine\n",
            "running SunOS 4.1.3  The problem is that when I run the sample .pov files and\n",
            "use the EXACT same parameters when compiling diff...\n",
            "\n",
            "Los 5 documentos más similares:\n",
            "  1. Índice 3444, Similaridad: 0.2305, Clase: comp.graphics\n",
            "     Texto: Hi, I'm just getting into PoVRay and I was wondering if there is a graphic\n",
            "package that outputs .POV files.  Any help would be appreciated.\n",
            "Thanks.\n",
            "\n",
            "L...\n",
            "  2. Índice 5799, Similaridad: 0.2091, Clase: comp.graphics\n",
            "     Texto: I finally got a 24 bit viewer for my POVRAY generated .TGA files.\n",
            "\n",
            "It was written in C by Sean Malloy and he kindly sent me a copy.  He\n",
            "wrote it for t...\n",
            "  3. Índice 5905, Similaridad: 0.1982, Clase: comp.graphics\n",
            "     Texto: \n",
            "Hallo POV-Renderers !\n",
            "I've got a BocaX3 Card. Now I try to get POV displaying True Colors\n",
            "while rendering. I've tried most of the options and UNIVESA...\n",
            "  4. Índice 1764, Similaridad: 0.1838, Clase: comp.graphics\n",
            "     Texto: hi guys\n",
            " like all people in this group i'm a fans of fractal and render sw\n",
            " my favourite are fractint pov & 3dstudio 2.0 \n",
            " now listen my ideas\n",
            " i'have...\n",
            "  5. Índice 3364, Similaridad: 0.1659, Clase: comp.graphics\n",
            "     Texto: Does anyone know of a good way (standard PC application/PD utility) to\n",
            "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
            "do t...\n",
            "\n",
            "Análisis de clases:\n",
            "  - Clase del documento base: comp.graphics\n",
            "  - Coincidencias exactas: 5/5\n",
            "  - Porcentaje de acierto: 100.0%\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Documento base (índice 4506):\n",
            "Clase: rec.autos\n",
            "Texto: \n",
            "This does sound good, but I heard it tends to leave more grit, etc in the \n",
            "oil pan.  Also, I've been told to change the old when it's hot before the\n",
            "grit has much time to settle.\n",
            "\n",
            "Any opinions?\n",
            "...\n",
            "\n",
            "Los 5 documentos más similares:\n",
            "  1. Índice 4211, Similaridad: 0.1894, Clase: rec.motorcycles\n",
            "     Texto: \n",
            "\n",
            "It's normal for the BMW K bikes to use a little oil in the first few thousand \n",
            "miles.  I don't know why.  I've had three new K bikes, and all three ...\n",
            "  2. Índice 5928, Similaridad: 0.1682, Clase: comp.sys.mac.hardware\n",
            "     Texto: or\n",
            "there\n",
            "\n",
            "\n",
            "Okay, I guess its time for a quick explanation of Mac sound.\n",
            "\n",
            "The original documentation for the sound hardware (IM-3) documents how to\n",
            "mak...\n",
            "  3. Índice 6224, Similaridad: 0.1583, Clase: rec.autos\n",
            "     Texto: Archive-name: rec-autos/part5\n",
            "\n",
            "[this article is one of a pair of articles containing commonly\n",
            "asked automotive questions; the other article contains q...\n",
            "  4. Índice 5171, Similaridad: 0.1577, Clase: rec.autos\n",
            "     Texto: \n",
            "If the tire has a leak you should fix it. \n",
            "\n",
            "\n",
            "Doesn't work too well if the engine is hot, its more accurate to check the\n",
            "oil when the engine is cool, ...\n",
            "  5. Índice 9491, Similaridad: 0.1522, Clase: rec.autos\n",
            "     Texto: My friend brought a subaru SVX recently.  I had drove it for couples times and I\n",
            "think its a great car, esp on snow.  However when she took it to a lo...\n",
            "\n",
            "Análisis de clases:\n",
            "  - Clase del documento base: rec.autos\n",
            "  - Coincidencias exactas: 3/5\n",
            "  - Porcentaje de acierto: 60.0%\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Documento base (índice 4012):\n",
            "Clase: rec.sport.hockey\n",
            "Texto: For those Leaf fans who are concerned, the following players are slated for\n",
            "return on Thursday's Winnipeg-Toronto game :\n",
            "    Peter Zezel, John Cullen\n",
            "\n",
            "  Mark Osborne and Dave Ellett are questionable t...\n",
            "\n",
            "Los 5 documentos más similares:\n",
            "  1. Índice 6599, Similaridad: 0.1600, Clase: soc.religion.christian\n",
            "     Texto: True.\n",
            "\n",
            "Also read 2 Peter 3:16\n",
            "\n",
            "Peter warns that the scriptures are often hard to understand by those who\n",
            "are not learned on the subject....\n",
            "  2. Índice 10644, Similaridad: 0.1428, Clase: rec.sport.hockey\n",
            "     Texto: In  <1qvos8$r78@cl.msu.>, vergolin@euler.lbs.msu.edu (David Vergolini) writes...\n",
            "\n",
            "There's quite a few Wings fans lurking about here, they just tend\n",
            "to...\n",
            "  3. Índice 7478, Similaridad: 0.1358, Clase: rec.sport.hockey\n",
            "     Texto: Toronto                          1 1 1--3\n",
            "Detroit                          1 4 1--6\n",
            "First period\n",
            "     1, Detroit, Yzerman 1 (Gallant, Ciccarelli) 4:48...\n",
            "  4. Índice 7308, Similaridad: 0.1318, Clase: rec.sport.hockey\n",
            "     Texto: Detroit is a very disciplined team.  There's a lot of Europeans\n",
            "in Detroit which would make the game fast, so Toronto would have\n",
            "to slow the game down...\n",
            "  5. Índice 10792, Similaridad: 0.1307, Clase: rec.sport.baseball\n",
            "     Texto: \n",
            "\n",
            "The tribe will be in town from April 16 to the 19th.\n",
            "There are ALWAYS tickets available! (Though they are playing Toronto,\n",
            "and many Toronto fans mak...\n",
            "\n",
            "Análisis de clases:\n",
            "  - Clase del documento base: rec.sport.hockey\n",
            "  - Coincidencias exactas: 3/5\n",
            "  - Porcentaje de acierto: 60.0%\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "je5kxvQMDLvf",
        "outputId": "4962e55f-5522-451f-cab2-9c914b196f5b",
        "jupyter": {
          "is_executing": true
        },
        "ExecuteTime": {
          "start_time": "2025-06-19T19:39:25.091989Z"
        }
      },
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"2. ENTRENAMIENTO Y OPTIMIZACIÓN DE MODELOS NAÏVE BAYES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"2.1 Modelos baseline:\")\n",
        "\n",
        "clf_nb = MultinomialNB()\n",
        "clf_nb.fit(X_train, y_train)\n",
        "\n",
        "clf_cnb = ComplementNB()\n",
        "clf_cnb.fit(X_train, y_train)\n",
        "\n",
        "X_test = tfidfvect.transform(newsgroups_test.data)\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "y_pred_nb = clf_nb.predict(X_test)\n",
        "y_pred_cnb = clf_cnb.predict(X_test)\n",
        "\n",
        "f1_nb_baseline = f1_score(y_test, y_pred_nb, average='macro')\n",
        "f1_cnb_baseline = f1_score(y_test, y_pred_cnb, average='macro')\n",
        "\n",
        "print(f\"Multinomial NB (baseline): F1-score = {f1_nb_baseline:.4f}\")\n",
        "print(f\"Complement NB (baseline): F1-score = {f1_cnb_baseline:.4f}\")\n",
        "\n",
        "print(\"\\n2.2 Optimización de hiperparámetros:\")\n",
        "\n",
        "param_grid = {\n",
        "    'tfidf__max_df': [0.75, 1.0],\n",
        "    'tfidf__min_df': [1, 2],\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
        "    'tfidf__stop_words': [None, 'english'],\n",
        "    'tfidf__sublinear_tf': [True, False],\n",
        "    'clf__alpha': [0.001, 0.01, 0.1, 1.0]\n",
        "}\n",
        "\n",
        "pipeline_nb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "pipeline_cnb = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer()),\n",
        "    ('clf', ComplementNB())\n",
        "])\n",
        "\n",
        "print(\"Optimizando Multinomial NB...\")\n",
        "grid_nb = GridSearchCV(\n",
        "    pipeline_nb,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "grid_nb.fit(newsgroups_train.data, y_train)\n",
        "\n",
        "print(\"Optimizando Complement NB...\")\n",
        "grid_cnb = GridSearchCV(\n",
        "    pipeline_cnb,\n",
        "    param_grid,\n",
        "    cv=3,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "grid_cnb.fit(newsgroups_train.data, y_train)\n",
        "\n",
        "print(f\"\\nResultados de optimización:\")\n",
        "print(f\"Mejor Multinomial NB: F1-score = {grid_nb.best_score_:.4f}\")\n",
        "print(f\"Mejor Complement NB: F1-score = {grid_cnb.best_score_:.4f}\")\n",
        "\n",
        "print(f\"\\nMejores parámetros Multinomial NB:\")\n",
        "for param, value in grid_nb.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nMejores parámetros Complement NB:\")\n",
        "for param, value in grid_cnb.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "y_pred_nb_opt = grid_nb.predict(newsgroups_test.data)\n",
        "y_pred_cnb_opt = grid_cnb.predict(newsgroups_test.data)\n",
        "\n",
        "f1_nb_opt = f1_score(y_test, y_pred_nb_opt, average='macro')\n",
        "f1_cnb_opt = f1_score(y_test, y_pred_cnb_opt, average='macro')\n",
        "\n",
        "print(f\"\\nEvaluación final en conjunto de prueba:\")\n",
        "print(f\"Multinomial NB optimizado: F1-score = {f1_nb_opt:.4f}\")\n",
        "print(f\"Complement NB optimizado: F1-score = {f1_cnb_opt:.4f}\")\n",
        "\n",
        "if f1_cnb_opt >= f1_nb_opt:\n",
        "    mejor_modelo = grid_cnb\n",
        "    mejor_nombre = \"Complement NB\"\n",
        "    mejor_f1 = f1_cnb_opt\n",
        "    mejor_pred = y_pred_cnb_opt\n",
        "else:\n",
        "    mejor_modelo = grid_nb\n",
        "    mejor_nombre = \"Multinomial NB\"\n",
        "    mejor_f1 = f1_nb_opt\n",
        "    mejor_pred = y_pred_nb_opt\n",
        "\n",
        "print(f\"\\nMejor modelo: {mejor_nombre} con F1-score = {mejor_f1:.4f}\")\n",
        "\n",
        "print(f\"\\nReporte de clasificación detallado ({mejor_nombre}):\")\n",
        "print(classification_report(y_test, mejor_pred, target_names=newsgroups_train.target_names))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "2. ENTRENAMIENTO Y OPTIMIZACIÓN DE MODELOS NAÏVE BAYES\n",
            "================================================================================\n",
            "2.1 Modelos baseline:\n",
            "Multinomial NB (baseline): F1-score = 0.5854\n",
            "Complement NB (baseline): F1-score = 0.6930\n",
            "\n",
            "2.2 Optimización de hiperparámetros:\n",
            "Optimizando Multinomial NB...\n",
            "Optimizando Complement NB...\n",
            "\n",
            "Resultados de optimización:\n",
            "Mejor Multinomial NB: F1-score = 0.7519\n",
            "Mejor Complement NB: F1-score = 0.7670\n",
            "\n",
            "Mejores parámetros Multinomial NB:\n",
            "  clf__alpha: 0.01\n",
            "  tfidf__max_df: 0.75\n",
            "  tfidf__min_df: 1\n",
            "  tfidf__ngram_range: (1, 2)\n",
            "  tfidf__stop_words: english\n",
            "  tfidf__sublinear_tf: False\n",
            "\n",
            "Mejores parámetros Complement NB:\n",
            "  clf__alpha: 0.1\n",
            "  tfidf__max_df: 0.75\n",
            "  tfidf__min_df: 1\n",
            "  tfidf__ngram_range: (1, 2)\n",
            "  tfidf__stop_words: english\n",
            "  tfidf__sublinear_tf: True\n",
            "\n",
            "Evaluación final en conjunto de prueba:\n",
            "Multinomial NB optimizado: F1-score = 0.6875\n",
            "Complement NB optimizado: F1-score = 0.7078\n",
            "\n",
            "Mejor modelo: Complement NB con F1-score = 0.7078\n",
            "\n",
            "Reporte de clasificación detallado (Complement NB):\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.34      0.47      0.39       319\n",
            "           comp.graphics       0.74      0.73      0.73       389\n",
            " comp.os.ms-windows.misc       0.70      0.59      0.64       394\n",
            "comp.sys.ibm.pc.hardware       0.66      0.70      0.68       392\n",
            "   comp.sys.mac.hardware       0.80      0.71      0.75       385\n",
            "          comp.windows.x       0.79      0.81      0.80       395\n",
            "            misc.forsale       0.77      0.81      0.79       390\n",
            "               rec.autos       0.82      0.73      0.77       396\n",
            "         rec.motorcycles       0.82      0.78      0.80       398\n",
            "      rec.sport.baseball       0.93      0.84      0.88       397\n",
            "        rec.sport.hockey       0.87      0.94      0.91       399\n",
            "               sci.crypt       0.76      0.81      0.78       396\n",
            "         sci.electronics       0.74      0.57      0.64       393\n",
            "                 sci.med       0.80      0.79      0.79       396\n",
            "               sci.space       0.78      0.79      0.79       394\n",
            "  soc.religion.christian       0.58      0.88      0.70       398\n",
            "      talk.politics.guns       0.60      0.71      0.65       364\n",
            "   talk.politics.mideast       0.78      0.84      0.81       376\n",
            "      talk.politics.misc       0.68      0.45      0.55       310\n",
            "      talk.religion.misc       0.50      0.22      0.30       251\n",
            "\n",
            "                accuracy                           0.72      7532\n",
            "               macro avg       0.72      0.71      0.71      7532\n",
            "            weighted avg       0.73      0.72      0.72      7532\n",
            "\n",
            "\n",
            "================================================================================\n",
            "3. ANÁLISIS DE SIMILARIDAD ENTRE PALABRAS\n",
            "================================================================================\n",
            "Transponiendo matriz documento-término...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'GridSearchCV' object has no attribute 'named_steps'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-3811277732.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m# Usar el mejor vectorizador encontrado\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m \u001b[0mmejor_vectorizador\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmejor_modelo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tfidf'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0mX_train_best\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmejor_vectorizador\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewsgroups_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mX_term_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_best\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'named_steps'"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"3. ANÁLISIS DE SIMILARIDAD ENTRE PALABRAS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"Transponiendo matriz documento-término...\")\n",
        "\n",
        "mejor_vectorizador = mejor_modelo.best_estimator_.named_steps['tfidf']\n",
        "X_train_best = mejor_vectorizador.fit_transform(newsgroups_train.data)\n",
        "X_term_doc = X_train_best.T\n",
        "\n",
        "print(f\"Forma de la matriz término-documento: {X_term_doc.shape}\")\n",
        "\n",
        "vocab_best = mejor_vectorizador.vocabulary_\n",
        "idx2word_best = {v: k for k, v in vocab_best.items()}\n",
        "\n",
        "print(f\"Forma de la matriz término-documento: {X_term_doc.shape}\")\n",
        "\n",
        "vocab_best = mejor_vectorizador.vocabulary_\n",
        "idx2word_best = {v: k for k, v in vocab_best.items()}\n",
        "\n",
        "def analizar_similaridad_palabra(palabra, nmost=5):\n",
        "    \"\"\"\n",
        "    Analiza la similaridad de una palabra con el resto del vocabulario\n",
        "    \"\"\"\n",
        "    if palabra not in vocab_best:\n",
        "        print(f\"La palabra '{palabra}' no está en el vocabulario.\")\n",
        "        return None, None\n",
        "\n",
        "    word_idx = vocab_best[palabra]\n",
        "\n",
        "    word_vector = X_term_doc[word_idx].reshape(1, -1)\n",
        "    similarities = cosine_similarity(word_vector, X_term_doc)[0]\n",
        "\n",
        "    similarities[word_idx] = -1\n",
        "\n",
        "    most_similar_idx = np.argsort(similarities)[-nmost:][::-1]\n",
        "\n",
        "    print(f\"\\nPalabra base: '{palabra}'\")\n",
        "    print(f\"Las {nmost} palabras más similares:\")\n",
        "\n",
        "    similar_words = []\n",
        "    similar_scores = []\n",
        "\n",
        "    for i, idx in enumerate(most_similar_idx):\n",
        "        similar_word = idx2word_best[idx]\n",
        "        similarity = similarities[idx]\n",
        "        similar_words.append(similar_word)\n",
        "        similar_scores.append(similarity)\n",
        "        print(f\"  {i+1}. '{similar_word}' (similaridad: {similarity:.4f})\")\n",
        "\n",
        "    return similar_words, similar_scores\n",
        "\n",
        "palabras_seleccionadas = ['computer', 'god', 'car', 'space', 'government']\n",
        "\n",
        "print(f\"Palabras seleccionadas para análisis: {palabras_seleccionadas}\")\n",
        "\n",
        "resultados_palabras = {}\n",
        "for palabra in palabras_seleccionadas:\n",
        "    words, scores = analizar_similaridad_palabra(palabra)\n",
        "    if words is not None:\n",
        "        resultados_palabras[palabra] = {\n",
        "            'similar_words': words,\n",
        "            'similarities': scores\n",
        "        }\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDl610eRBgUY",
        "outputId": "701e117b-a6f4-443e-c424-bb6f831780d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "3. ANÁLISIS DE SIMILARIDAD ENTRE PALABRAS\n",
            "================================================================================\n",
            "Transponiendo matriz documento-término...\n",
            "Forma de la matriz término-documento: (943737, 11314)\n",
            "Forma de la matriz término-documento: (943737, 11314)\n",
            "Palabras seleccionadas para análisis: ['computer', 'god', 'car', 'space', 'government']\n",
            "\n",
            "Palabra base: 'computer'\n",
            "Las 5 palabras más similares:\n",
            "  1. 'computer science' (similaridad: 0.2456)\n",
            "  2. 'computer graphics' (similaridad: 0.2206)\n",
            "  3. 'turn computer' (similaridad: 0.1745)\n",
            "  4. 'new computer' (similaridad: 0.1743)\n",
            "  5. 'computer shops' (similaridad: 0.1662)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Palabra base: 'god'\n",
            "Las 5 palabras más similares:\n",
            "  1. 'jesus' (similaridad: 0.3080)\n",
            "  2. 'christ' (similaridad: 0.2682)\n",
            "  3. 'faith' (similaridad: 0.2673)\n",
            "  4. 'bible' (similaridad: 0.2633)\n",
            "  5. 'believe god' (similaridad: 0.2586)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Palabra base: 'car'\n",
            "Las 5 palabras más similares:\n",
            "  1. 'new car' (similaridad: 0.2309)\n",
            "  2. 'bought car' (similaridad: 0.2168)\n",
            "  3. 'cars' (similaridad: 0.1949)\n",
            "  4. 'car car' (similaridad: 0.1944)\n",
            "  5. 'car like' (similaridad: 0.1910)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Palabra base: 'space'\n",
            "Las 5 palabras más similares:\n",
            "  1. 'space station' (similaridad: 0.3098)\n",
            "  2. 'sci space' (similaridad: 0.2824)\n",
            "  3. 'space shuttle' (similaridad: 0.2696)\n",
            "  4. 'nasa' (similaridad: 0.2662)\n",
            "  5. 'shuttle' (similaridad: 0.2287)\n",
            "------------------------------------------------------------\n",
            "\n",
            "Palabra base: 'government'\n",
            "Las 5 palabras más similares:\n",
            "  1. 'government agencies' (similaridad: 0.2280)\n",
            "  2. 'federal government' (similaridad: 0.2131)\n",
            "  3. 'encryption' (similaridad: 0.1999)\n",
            "  4. 'agencies' (similaridad: 0.1965)\n",
            "  5. 'limited government' (similaridad: 0.1956)\n",
            "------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkGJhetEPdA4",
        "outputId": "de2454b7-8046-4503-ef16-2774bb0cdce4"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "RESUMEN DE RESULTADOS\n",
            "================================================================================\n",
            "1. ANÁLISIS DE SIMILARIDAD ENTRE DOCUMENTOS:\n",
            "   - Se analizaron 5 documentos aleatorios\n",
            "   - La vectorización TF-IDF mostró ser efectiva para identificar documentos similares\n",
            "   - La mayoría de documentos similares pertenecían a clases relacionadas\n",
            "\n",
            "2. OPTIMIZACIÓN DE MODELOS NAÏVE BAYES:\n",
            "   - Modelo baseline MultinomialNB: F1-score = 0.5854\n",
            "   - Modelo baseline ComplementNB: F1-score = 0.6930\n",
            "   - Mejor modelo optimizado: Complement NB\n",
            "   - Mejor F1-score obtenido: 0.7078\n",
            "   - Mejora respecto al baseline: 0.0149\n",
            "\n",
            "3. ANÁLISIS DE SIMILARIDAD ENTRE PALABRAS:\n",
            "   - Se analizaron 5 palabras seleccionadas\n",
            "   - La matriz término-documento permitió identificar palabras semánticamente relacionadas\n",
            "   - Los resultados muestran la capacidad de capturar relaciones semánticas\n",
            "\n",
            "CONCLUSIONES GENERALES:\n",
            "- La vectorización TF-IDF es efectiva para representar documentos de texto\n",
            "- Los modelos Naïve Bayes muestran buen rendimiento en clasificación de texto\n",
            "- La optimización de hiperparámetros mejora significativamente el rendimiento\n",
            "- Las matrices transpuestas permiten analizar relaciones entre términos\n"
          ]
        }
      ],
      "execution_count": 8,
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RESUMEN DE RESULTADOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"1. ANÁLISIS DE SIMILARIDAD ENTRE DOCUMENTOS:\")\n",
        "print(f\"   - Se analizaron {len(random_docs)} documentos aleatorios\")\n",
        "print(\"   - La vectorización TF-IDF mostró ser efectiva para identificar documentos similares\")\n",
        "print(\"   - La mayoría de documentos similares pertenecían a clases relacionadas\")\n",
        "\n",
        "print(f\"\\n2. OPTIMIZACIÓN DE MODELOS NAÏVE BAYES:\")\n",
        "print(f\"   - Modelo baseline MultinomialNB: F1-score = {f1_nb_baseline:.4f}\")\n",
        "print(f\"   - Modelo baseline ComplementNB: F1-score = {f1_cnb_baseline:.4f}\")\n",
        "print(f\"   - Mejor modelo optimizado: {mejor_nombre}\")\n",
        "print(f\"   - Mejor F1-score obtenido: {mejor_f1:.4f}\")\n",
        "print(f\"   - Mejora respecto al baseline: {(mejor_f1 - max(f1_nb_baseline, f1_cnb_baseline)):.4f}\")\n",
        "\n",
        "print(f\"\\n3. ANÁLISIS DE SIMILARIDAD ENTRE PALABRAS:\")\n",
        "print(f\"   - Se analizaron {len(palabras_seleccionadas)} palabras seleccionadas\")\n",
        "print(\"   - La matriz término-documento permitió identificar palabras semánticamente relacionadas\")\n",
        "print(\"   - Los resultados muestran la capacidad de capturar relaciones semánticas\")\n",
        "\n",
        "print(f\"\\nCONCLUSIONES GENERALES:\")\n",
        "print(\"- La vectorización TF-IDF es efectiva para representar documentos de texto\")\n",
        "print(\"- Los modelos Naïve Bayes muestran buen rendimiento en clasificación de texto\")\n",
        "print(\"- La optimización de hiperparámetros mejora significativamente el rendimiento\")\n",
        "print(\"- Las matrices transpuestas permiten analizar relaciones entre términos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McArD4rSDR2K"
      },
      "source": [
        "### Consigna del desafío 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJgf6GQIIEH1"
      },
      "source": [
        "**1**. Vectorizar documentos. Tomar 5 documentos al azar y medir similaridad con el resto de los documentos.\n",
        "Estudiar los 5 documentos más similares de cada uno analizar si tiene sentido\n",
        "la similaridad según el contenido del texto y la etiqueta de clasificación.\n",
        "\n",
        "**2**. Entrenar modelos de clasificación Naïve Bayes para maximizar el desempeño de clasificación\n",
        "(f1-score macro) en el conjunto de datos de test. Considerar cambiar parámteros\n",
        "de instanciación del vectorizador y los modelos y probar modelos de Naïve Bayes Multinomial\n",
        "y ComplementNB.\n",
        "\n",
        "**3**. Transponer la matriz documento-término. De esa manera se obtiene una matriz\n",
        "término-documento que puede ser interpretada como una colección de vectorización de palabras.\n",
        "Estudiar ahora similaridad entre palabras tomando 5 palabras y estudiando sus 5 más similares. **La elección de palabras no debe ser al azar para evitar la aparición de términos poco interpretables, elegirlas \"manualmente\"**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLNaUvN_tzD9"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}